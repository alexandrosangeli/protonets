{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-layer perceptron\n",
    "\n",
    "This notebook is used during testing/validation.\n",
    "\n",
    "See `src/mlp.py` for the clean version of the MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value:\n",
    "\n",
    "    def __init__(self, data, children=[], label=''):\n",
    "        self.data = data\n",
    "        self.grad = 0.0\n",
    "        self.label = label\n",
    "        self.prev = children\n",
    "        self._backward = lambda : None\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Value(data={self.data}, grad={self.grad})\"\n",
    "\n",
    "    def __add__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data + other.data, [self, other], f\"({self.label})+{other.label}\")\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += 1.0 * out.grad\n",
    "            other.grad += 1.0 * out.grad\n",
    "            \n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def __sub__(self, other):\n",
    "        out = Value(self.data - other.data, [self, other], f\"({self.label})+{other.label}\")\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += 1.0 * out.grad\n",
    "            other.grad += 1.0 * out.grad\n",
    "            \n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data * other.data, [self, other], f\"({self.label})*{other.label}\")\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += other.data * out.grad\n",
    "            other.grad += self.data * out.grad\n",
    "            \n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def __pow__(self, other):\n",
    "        out = Value(self.data ** other.data, [self, other], f\"({self.label})^{other.label}\")\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad += other.data * self.data * out.grad\n",
    "            sign = 1 if self.data > 0 else -1\n",
    "            other.grad += self.data ** other.data * sign * np.log(abs(self.data)) * out.grad\n",
    "            \n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def sig(self):\n",
    "        y = np.exp(self.data) / (np.exp(self.data) + 1)\n",
    "        out = Value(y, [self], f\"(sig({self.label})\")\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad += y * (1 - y)\n",
    "            \n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "\n",
    "    def backward(self):\n",
    "        topo = []\n",
    "        visited = set()\n",
    "        def topo_sort(node):\n",
    "            if node not in visited:\n",
    "                visited.add(node)\n",
    "                for child in node.prev:\n",
    "                    topo_sort(child)\n",
    "                topo.append(node)\n",
    "\n",
    "        self.grad = 1\n",
    "        topo_sort(self)\n",
    "        nodes = reversed(topo)\n",
    "        for node in nodes:\n",
    "            node._backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value(data=4.000027108143415, grad=1)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(5)\n",
    "class Neuron:\n",
    "\n",
    "    def __init__(self, num_input):\n",
    "        self.w = [Value(w) for w  in np.random.randn(num_input)]\n",
    "        self.b = Value(np.random.randn(1)[0])\n",
    "\n",
    "    def __call__(self, xs):\n",
    "        # out = np.dot(self.w, [Value(x) for x  in xs]) + self.b\n",
    "        # print([w*x for w, x in zip(self.w, xs)])\n",
    "        # out = sum([w*x for w, x in zip(self.w, xs)]) + self.b\n",
    "        out = np.dot(self.w, xs) + self.b\n",
    "        return out.sig()\n",
    "\n",
    "n = Neuron(3)\n",
    "target = Value(3)\n",
    "out = n([3,4,5])\n",
    "\n",
    "loss = (out - target) ** Value(2)\n",
    "loss.backward()\n",
    "\n",
    "n.w[0]\n",
    "loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "args: (2, 3, 4)\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "def f(g, *args):\n",
    "    print(\"args:\", args)\n",
    "    for x in args:\n",
    "        print(x)\n",
    "\n",
    "f(1,2,3,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Value(data=0.8206791118848349, grad=0.0), Value(data=0.0009815753433625469, grad=0.0), Value(data=0.0004586693901978576, grad=0.0), Value(data=0.9815206068237023, grad=0.0)]\n"
     ]
    }
   ],
   "source": [
    "class Layer:\n",
    "\n",
    "    def __init__(self, num_input, num_neurons):\n",
    "        # number of neurons is equivalent to number of outputs\n",
    "        self.num_input = num_input\n",
    "        self.num_neurons = num_neurons\n",
    "        self.neurons = [Neuron(num_input) for _ in range(num_neurons)]\n",
    "\n",
    "    def __call__(self, xs):\n",
    "        outputs = [n(xs) for n in self.neurons]\n",
    "        return outputs if len(outputs) > 1 else outputs[0]\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return  f\"Layer({self.num_input=}, {self.num_neurons=})\"\n",
    "    \n",
    "l = Layer(3,4)\n",
    "out = l([3,4,5])\n",
    "print(out)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value(data=0.37517498361981066, grad=0.0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MLP:\n",
    "\n",
    "    def __init__(self, num_inputs, outs):\n",
    "        # number of outputs is for each layers\n",
    "        self.layers = [Layer(num_inputs, outs[0])] + [Layer(outs[i], outs[i+1]) for i in range(len(outs) - 1)]\n",
    "\n",
    "    def __call__(self, xs):\n",
    "        outs = xs\n",
    "        for layer in self.layers:\n",
    "            outs = layer(outs)\n",
    "\n",
    "        return outs\n",
    "mlp = MLP(3, [3, 2, 1])\n",
    "mlp([4,1,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a.grad=96.0\n",
      "b.grad=72.0\n",
      "c.grad=357.8265575694721\n",
      "96.0000160148411\n",
      "72.00000899842962\n",
      "357.82700220465813\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "\n",
    "    a = Value(3, label='a')\n",
    "    b = Value(4, label='b')\n",
    "    c = Value(2, label='c')\n",
    "\n",
    "    g = (a * b)\n",
    "    h = g ** c\n",
    "    h.backward()\n",
    "    # h.grad = 1\n",
    "\n",
    "    # h._backward()\n",
    "    # g._backward()\n",
    "\n",
    "    print(f\"{a.grad=}\")\n",
    "    print(f\"{b.grad=}\")\n",
    "    print(f\"{c.grad=}\")\n",
    "\n",
    "    # test if grads make sense\n",
    "    e = 1e-6\n",
    "    for elt in np.diag([e]*3):\n",
    "        before = (a.data * b.data) ** c.data\n",
    "        after = ((a.data + elt[0]) * (b.data + elt[1])) ** (c.data + elt[2])\n",
    "        print((after - before)/e)\n",
    "\n",
    "main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
