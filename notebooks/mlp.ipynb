{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-layer perceptron\n",
    "\n",
    "This notebook is used during testing/validation.\n",
    "\n",
    "See `src/mlp.py` for the clean version of the MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value:\n",
    "\n",
    "    def __init__(self, data, children=[], label=''):\n",
    "        self.data = data\n",
    "        self.grad = 0.0\n",
    "        self.label = label\n",
    "        self.prev = children\n",
    "        self._backward = lambda : None\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Value(data={self.data}, grad={self.grad})\"\n",
    "\n",
    "    def __add__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data + other.data, [self, other], f\"({self.label})+{other.label}\")\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += 1.0 * out.grad\n",
    "            other.grad += 1.0 * out.grad\n",
    "            \n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def __sub__(self, other):\n",
    "        out = Value(self.data - other.data, [self, other], f\"({self.label})+{other.label}\")\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += 1.0 * out.grad\n",
    "            other.grad += 1.0 * out.grad\n",
    "            \n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data * other.data, [self, other], f\"({self.label})*{other.label}\")\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += other.data * out.grad\n",
    "            other.grad += self.data * out.grad\n",
    "            \n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def __pow__(self, other):\n",
    "        out = Value(self.data ** other.data, [self, other], f\"({self.label})^{other.label}\")\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad += other.data * self.data * out.grad\n",
    "            sign = 1 if self.data > 0 else -1\n",
    "            other.grad += self.data ** other.data * sign * np.log(abs(self.data)) * out.grad\n",
    "            \n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def sig(self):\n",
    "        y = 1 / (np.exp(-self.data) + 1)\n",
    "        out = Value(y, [self], f\"(sig({self.label})\")\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad += y * (1 - y) * out.grad\n",
    "            \n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def relu(self):\n",
    "        out = Value(0 if self.data < 0 else self.data, (self,), 'ReLU')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += (out.data > 0) * out.grad\n",
    "\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def tanh(self):\n",
    "        out = Value((np.exp(2*self.data) - 1) / (np.exp(2*self.data) + 1), [self], f\"(tanh({self.label})\")\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += (1 - out.data**2) * out.grad\n",
    "\n",
    "        out._backward = _backward \n",
    "        return out\n",
    "    \n",
    "    def backward(self):\n",
    "        topo = []\n",
    "        visited = set()\n",
    "        def topo_sort(node):\n",
    "            if node not in visited:\n",
    "                visited.add(node)\n",
    "                for child in node.prev:\n",
    "                    topo_sort(child)\n",
    "                topo.append(node)\n",
    "\n",
    "        self.grad = 1\n",
    "        topo_sort(self)\n",
    "        nodes = reversed(topo)\n",
    "        for node in nodes:\n",
    "            node._backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out_before: Value(data=0.5888810564717789, grad=1)\n",
      "Gradient using backwards: -2.8245869576938945e-06\n",
      "out_after: Value(data=0.5888810564435326, grad=0.0)\n",
      "Gradient by hand: -2.824629419251323e-06\n"
     ]
    }
   ],
   "source": [
    "# validate the gradients\n",
    "mlp = MLP(3, [3, 2, 1])\n",
    "\n",
    "out_before = mlp([4,1,5])\n",
    "out_before.backward()\n",
    "\n",
    "print(\"out_before:\", out_before)\n",
    "grad = mlp.layers[0].neurons[0].w[0].grad\n",
    "print(\"Gradient using backwards:\", grad)\n",
    "\n",
    "h = 1e-5\n",
    "mlp.layers[0].neurons[0].w[0].data += h\n",
    "\n",
    "\n",
    "out_after = mlp([4,1,5])\n",
    "print(\"out_after:\", out_after)\n",
    "\n",
    "\n",
    "a = out_after.data\n",
    "b = out_before.data\n",
    "manual_grad = (a - b)/h\n",
    "print(\"Gradient by hand:\", manual_grad)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value(data=4.000027108143415, grad=1)"
      ]
     },
     "execution_count": 402,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(5)\n",
    "class Neuron:\n",
    "\n",
    "    def __init__(self, num_input):\n",
    "        self.w = [Value(w) for w  in np.random.randn(num_input)]\n",
    "        self.b = Value(np.random.randn(1)[0])\n",
    "\n",
    "    def __call__(self, xs):\n",
    "        # out = np.dot(self.w, [Value(x) for x  in xs]) + self.b\n",
    "        # print([w*x for w, x in zip(self.w, xs)])\n",
    "        # out = sum([w*x for w, x in zip(self.w, xs)]) + self.b\n",
    "        out = np.dot(self.w, xs) + self.b\n",
    "        return out.sig()\n",
    "\n",
    "n = Neuron(3)\n",
    "target = Value(3)\n",
    "out = n([3,4,5])\n",
    "\n",
    "loss = (out - target) ** Value(2)\n",
    "loss.backward()\n",
    "\n",
    "n.w[0]\n",
    "loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Value(data=0.8206791118848349, grad=0.0), Value(data=0.0009815753433625469, grad=0.0), Value(data=0.0004586693901978576, grad=0.0), Value(data=0.9815206068237023, grad=0.0)]\n"
     ]
    }
   ],
   "source": [
    "class Layer:\n",
    "\n",
    "    def __init__(self, num_input, num_neurons):\n",
    "        # number of neurons is equivalent to number of outputs\n",
    "        self.num_input = num_input\n",
    "        self.num_neurons = num_neurons\n",
    "        self.neurons = [Neuron(num_input) for _ in range(num_neurons)]\n",
    "\n",
    "    def __call__(self, xs):\n",
    "        outputs = [n(xs) for n in self.neurons]\n",
    "        return outputs if len(outputs) > 1 else outputs[0]\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return  f\"Layer({self.num_input=}, {self.num_neurons=})\"\n",
    "    \n",
    "l = Layer(3,4)\n",
    "out = l([3,4,5])\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value(data=0, grad=0.0)\n",
      "[Value(data=-0.3633108784819174, grad=0.0), Value(data=0.0032888429341100755, grad=0.0), Value(data=-0.10593044205742323, grad=0.0), Value(data=-0.6315716297922155, grad=0.0), Value(data=-0.0061949084857593475, grad=0.0), Value(data=-0.10106761180924467, grad=0.0), Value(data=0.24921765856490757, grad=0.0), Value(data=0.19766009104249851, grad=0.0), Value(data=1.3348485742415819, grad=0.0), Value(data=1.5615322934488904, grad=0.0), Value(data=-0.3058530211666308, grad=0.0), Value(data=-0.47773141727821256, grad=0.0), Value(data=0.3554384723493521, grad=0.0), Value(data=0.269612406446701, grad=0.0), Value(data=1.2919633833879631, grad=0.0), Value(data=0.49444039812108825, grad=0.0), Value(data=-0.3363362591365529, grad=0.0)]\n",
      "Value(data=0, grad=0.0)\n",
      "[Value(data=-0.1917395684517186, grad=0.0), Value(data=0.0032888429341100755, grad=0.0), Value(data=-0.10593044205742323, grad=0.0), Value(data=-0.6315716297922155, grad=0.0), Value(data=-0.0061949084857593475, grad=0.0), Value(data=-0.10106761180924467, grad=0.0), Value(data=0.24921765856490757, grad=0.0), Value(data=0.19766009104249851, grad=0.0), Value(data=1.3348485742415819, grad=0.0), Value(data=1.5615322934488904, grad=0.0), Value(data=-0.3058530211666308, grad=0.0), Value(data=-0.47773141727821256, grad=0.0), Value(data=0.3554384723493521, grad=0.0), Value(data=0.269612406446701, grad=0.0), Value(data=1.2919633833879631, grad=0.0), Value(data=0.49444039812108825, grad=0.0), Value(data=-0.3363362591365529, grad=0.0)]\n"
     ]
    }
   ],
   "source": [
    "class MLP:\n",
    "\n",
    "    def __init__(self, num_inputs, outs):\n",
    "        # number of outputs is for each layers\n",
    "        self.layers = [Layer(num_inputs, outs[0])] + [Layer(outs[i], outs[i+1]) for i in range(len(outs) - 1)]\n",
    "\n",
    "    def __call__(self, xs):\n",
    "        outs = xs\n",
    "        for layer in self.layers:\n",
    "            outs = layer(outs)\n",
    "        return outs\n",
    "    \n",
    "    def parameters(self):\n",
    "        params = []\n",
    "        for layer in self.layers:\n",
    "            for neuron in layer.neurons:\n",
    "                for weight in neuron.w:\n",
    "                    params.append(weight)\n",
    "        return params\n",
    "    \n",
    "    \n",
    "mlp = MLP(3, [3, 2, 1])\n",
    "print(mlp([4,1,5]))\n",
    "print(mlp.parameters())\n",
    "\n",
    "mlp.layers[0].neurons[0].w[0].data= -0.1917395684517186\n",
    "print(mlp([4,1,5]))\n",
    "\n",
    "\n",
    "print(mlp.parameters())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out_before: Value(data=-0.6939227666304026, grad=1)\n",
      "Gradient using backwards: 0.10724593739600777\n",
      "out_after: Value(data=-0.6939216941140519, grad=0.0)\n",
      "Gradient by hand: 0.1072516350730801\n"
     ]
    }
   ],
   "source": [
    "# validate the gradients\n",
    "mlp = MLP(3, [3, 2, 1])\n",
    "\n",
    "out_before = mlp([4,1,5])\n",
    "out_before.backward()\n",
    "\n",
    "print(\"out_before:\", out_before)\n",
    "grad = mlp.layers[0].neurons[0].w[0].grad\n",
    "print(\"Gradient using backwards:\", grad)\n",
    "\n",
    "h = 1e-5\n",
    "mlp.layers[0].neurons[0].w[0].data += h\n",
    "\n",
    "\n",
    "out_after = mlp([4,1,5])\n",
    "print(\"out_after:\", out_after)\n",
    "\n",
    "\n",
    "a = out_after.data\n",
    "b = out_before.data\n",
    "manual_grad = (a - b)/h\n",
    "print(\"Gradient by hand:\", manual_grad)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a.grad=96.0\n",
      "b.grad=72.0\n",
      "c.grad=357.8265575694721\n",
      "96.0000160148411\n",
      "72.00000899842962\n",
      "357.82700220465813\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "\n",
    "    a = Value(3, label='a')\n",
    "    b = Value(4, label='b')\n",
    "    c = Value(2, label='c')\n",
    "\n",
    "    g = (a * b)\n",
    "    h = g ** c\n",
    "    h.backward()\n",
    "    # h.grad = 1\n",
    "\n",
    "    # h._backward()\n",
    "    # g._backward()\n",
    "\n",
    "    print(f\"{a.grad=}\")\n",
    "    print(f\"{b.grad=}\")\n",
    "    print(f\"{c.grad=}\")\n",
    "\n",
    "    # test if grads make sense\n",
    "    e = 1e-6\n",
    "    for elt in np.diag([e]*3):\n",
    "        before = (a.data * b.data) ** c.data\n",
    "        after = ((a.data + elt[0]) * (b.data + elt[1])) ** (c.data + elt[2])\n",
    "        print((after - before)/e)\n",
    "\n",
    "main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
